{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell below loads all libraries used within this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import gzip\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import arviz as az\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method was used from within the utils folder of the dataset to load different datasets and images withing fashion-MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    \n",
    "    ''' from https://github.com/zalandoresearch/fashion-mnist'''\n",
    "\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle components analysis or PCA, is a technique for reducing the dimensionality of datasets. The aim is to increase a databases interpretability while minimizing information loss. This is achieved by creating new uncorrelated variables that successively maximize variance. in the cell below, the dataset is loaded and then standardized by dividing them by 225 since they are all pixel values. Furthermore, their mean is calculated, and the data is normalized to make the PCA appliance easier. The first 25 images are shown with matching labels in the block below. Method 'conv_2d' splits the 1d input of 784 pixels into 2d with the shape (28,28) to make the plotting possible.\n",
    "\n",
    "There are 10 unique classes within this dataset which are : 0-t-shirt/top', 1-'Trouser', 2-'Pullover', 3-'Dress', 4-'Coat', 5-'Sandal', 6-'Shirt', 7-'Sneaker', 8-'bag', 9-'ankle boot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_X_train, train_y_labels = load_mnist('C:/Uni/Year3/MLCW//fashion-mnist/data/fashion', kind='train')\n",
    "fashion_X_test, test_y_labels = load_mnist('C:/Uni/Year3/MLCW/fashion-mnist/data/fashion', kind='t10k')\n",
    "\n",
    "class_names = ['t-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'bag', 'ankle boot']\n",
    "\n",
    "x_train_orig = fashion_X_train\n",
    "x_test_orig = fashion_X_test\n",
    "#standardization: devided by 255\n",
    "x_train_standard = fashion_X_train/255\n",
    "x_test_standard = fashion_X_test/255\n",
    "\n",
    "#normalizing by finding mean and calculating distance from mean\n",
    "train_mean = np.mean(x_train_standard)\n",
    "test_mean = np.mean(x_test_standard)\n",
    "\n",
    "train_normal = x_train_standard - train_mean\n",
    "test_normal = x_test_standard - test_mean\n",
    "\n",
    "def conv_2d(matrix):\n",
    "    for x in matrix:\n",
    "        matrix = np.reshape(matrix, (28,28))\n",
    "    return matrix\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    train_images = conv_2d(fashion_X_train[i])\n",
    "    plt.imshow(train_images, cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_y_labels[i]], c='red')\n",
    "\n",
    "\n",
    "plt.savefig('first25ImagesOfFashion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensionality of the data is 784, consistant of 28x28 pixels. PCA is performed here to understand whether this reduction significantly impacts our information retention or not. a good reduction will aim to keep most of the information while discarding as many dimensions as possible. Here, we learn some quantities about these data namely different components and cumulative variance of the components. These represent the principal components of the data. in the code cell below evector of >=0.8 is taken from the cumulative variance as we want to retain 80+% of our datapoints as it is the ideal number. This is done to gain an understanding of what would be an ideal reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "fit_all = pca.fit_transform(train_normal)\n",
    "\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "cumulative_ratio = np.cumsum(explained_var)\n",
    "less_eq_80 = np.where(cumulative_ratio >= 0.8)[0][0] #evector with cumulative variance ratio of >=0.8\n",
    "\n",
    "print(\"ideal pc retention = \"+str(less_eq_80+1))\n",
    "print(f\"first 5 PCs: {pca.explained_variance_[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA was applied to all data and the pca object of the first 10 principles were returned. With this analysis we intend to keep higher variances. The output tells us that the cumalitive variance decreases rapidly between first 3 components. Furthermore, to understand how many dimension would be ideal to retain, Cumulative Variance Ratio per Principal Component is plotted. This better visualizes the pca of whole data. It can be observed that the first \"51\" principle components can explain 80% of the variance in this dataset. so reducing the datasets to 51 dimensions is ideal.\n",
    "\n",
    "in the cell below we reduce the dimensionality of our data to 2 components. the data is then visualized as we plot the first component against the second component. Cumulative variance of the two components are also printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2 = PCA(n_components=2)\n",
    "fit_2d = pca_2.fit_transform(train_normal)\n",
    "explained_var = pca_2.explained_variance_#variance\n",
    "cumalitive_var = np.cumsum(explained_var)#cumulative var\n",
    "print(explained_var, 'pcs variance')\n",
    "print(cumalitive_var, ' cumulitative variance')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "scatter = plt.scatter(fit_2d.T[0], fit_2d.T[1], c=train_y_labels)\n",
    "clb = plt.colorbar(scatter) #colour coding\n",
    "clb.ax.set_title('Class') #colour coding\n",
    "plt.xlabel(\"1st principle comp\", c='red')\n",
    "plt.ylabel(\"2nd principle comp\", c='red')\n",
    "plt.title('2D pca', c='red')\n",
    "plt.savefig('2d_PCA_scatter', c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first principle component is \"19.80980567\" and the second principle component is \"31.92201614\" clearly showing the gap between the two components are high. This tells us that reducing to 2d from 780+ dimensions is not a good idea since all the classes keep clustering into each other and there is no clear separation between the classes and most of the datapoint are lost and we are underfitting our data. We can furthur prove that this reduction is not fitting with plotting our images with this reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize = (6,6), sharex=True, sharey = True, constrained_layout = True )\n",
    "\n",
    "# First 2 principal components\n",
    "p_2d = [pca.components_[i].reshape(28,28) for i in range(0,2)]\n",
    "\n",
    "for pc in range(0,2):\n",
    "    axes[0,pc].imshow(p_2d[0], aspect='auto', cmap = \"gray_r\")\n",
    "    axes[1,pc].imshow(p_2d[pc], aspect='auto', cmap = \"gray_r\")\n",
    "\n",
    "plt.savefig('2dPCA_reductionImage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is demonstrated that the images come out extremly blurry and do not really represeant all of the shapes we will see within our dataset. Moreover, these images show that most variance in the data is found between the pixel values of a dark shirt and the pixel values of a white shoe.\n",
    "\n",
    "in the code block below a simple K-means clustering is being employed. Each data is being assigned a cluster and cluster centres are calculated and marked inside each cluster. This is an unsupervised task with the task to separate points inside a dataset into different clusters such that elements within each cluster are similar. GMM is an example of soft clustering while K-mean is an example of hard clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10)\n",
    "label = kmeans.fit_predict(fit_2d)\n",
    "centroid = kmeans.cluster_centers_\n",
    "unique = np.unique(label)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in unique:\n",
    "    plt.scatter(fit_2d[label == i , 0] , fit_2d[label == i , 1] , label = i)\n",
    "    plt.scatter(centroid[:,0], centroid[:,1], s = 80, color = 'b')\n",
    "\n",
    "plt.savefig('Kmean_scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian mixture model is a probabilistic model that is assembled by different gaussian distributions with each having their own mean and standard deviation. in GMM, we do a weighted average of all of them to mix them into one gaussian distribution hence the name mixture model. Since GMM is a probabilistic model, it is possible to find probabilistic cluster assignments. Using \"fit_predict()\" to find clusters for a number of our data points, 300 to be exact. Predictions are then scattered into a plot for better visualization. \n",
    "Furthermore, the cluster centroids are represented by a grey X marker within the scatters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours = ['red', 'green', 'purple', 'yellow', 'blue', 'cyan', 'black', 'grey', 'orange', 'brown']\n",
    "\n",
    "gmm = GaussianMixture(n_components=10, random_state=0 ,init_params='kmeans')\n",
    "x = fit_2d\n",
    "mea = gmm.fit(x).means_\n",
    "means = np.mean(mea)\n",
    "pred_labels = gmm.fit_predict(x)\n",
    "score = gmm.score(x)\n",
    "std = np.std(x)\n",
    "print('std', std)\n",
    "print('mean', means)\n",
    "print('score', score)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "pred = plt.scatter(x[:, 0], x[:, 1], c=pred_labels, cmap='viridis', s=20) #predictions\n",
    "clb = plt.colorbar(pred) #colour coding\n",
    "clb.ax.set_title('Class labels') #colour coding\n",
    "plt.savefig('GMM_RS0')\n",
    "plt.show()\n",
    "\n",
    "centers = np.empty(shape=(gmm.n_components, x.shape[1]))\n",
    "mean_centre = gmm.means_\n",
    "cov_centre = gmm.covariances_\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in range(gmm.n_components):\n",
    "    density = multivariate_normal(mean_centre[i], cov_centre[i]).logpdf(x)\n",
    "    centers[i, :] = x[np.argmax(density)]\n",
    "plt.scatter(centers[:, 0], centers[:, 1], s=500, c='grey', marker='X')\n",
    "print(centers)\n",
    "plt.savefig('GMM_RS0_Centers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster centres are:\n",
    "\n",
    "[[ 3.8964778   1.3946751 ], [-3.2280289  -2.44768891], [ 2.2198116  -3.81102286], [-3.79174098  3.77144966], [-0.22498571 -5.51308641], [-6.25434046  0.76931686], [ 0.40075786  5.15356899]\n",
    "\n",
    "[-0.13584998  0.023941  ], [ 5.672128   -2.46995549], [ 6.96354057  2.74467447]].\n",
    "\n",
    "z-score for this model is \"-5.047949428065528\" with some minor overlapping happening between the classes. This score reveals that these datapoints are about 5 standard deviations away from the mean. Data's standard deviation is \"3.995089742715105\" and the mean is \"0.2550780733852641\". Since there is a large gap between the mean and standard deviation, it suggests that our data are scattered and not close to each other. It is worth mentioning that the random states in set to zero hence making it possible that the expectation maximization step(EM), has missed globally optimal solutions.\n",
    "\n",
    "Below we manipulate the data furthure by changing the number of random states present within our model to 42 for understanding whether this will have an impact or not. This variable, changes the test, train split, furthure splliting our data points into test and train subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=10, random_state=42 ,init_params='kmeans')\n",
    "x_42 = fit_2d\n",
    "mea_42 = gmm.fit(x_42).means_\n",
    "means_42 = np.mean(mea_42)\n",
    "pred_labels_42 = gmm.fit_predict(x)\n",
    "score_42 = gmm.score(x_42)\n",
    "std_42 = np.std(x)\n",
    "print('std', std_42)\n",
    "print('mean', means_42)\n",
    "print('score', score_42)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "pred = plt.scatter(x[:, 0], x[:, 1], c=pred_labels_42, cmap='viridis', s=20) #predictions\n",
    "clb = plt.colorbar(pred) #colour coding\n",
    "clb.ax.set_title('Class labels') #colour coding\n",
    "plt.savefig('GMM_RS42')\n",
    "plt.show()\n",
    "\n",
    "centers = np.empty(shape=(gmm.n_components, x.shape[1]))\n",
    "mean_centre = gmm.means_\n",
    "cov_centre = gmm.covariances_\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(gmm.n_components):\n",
    "    density = multivariate_normal(mean_centre[i], cov_centre[i]).logpdf(x)\n",
    "    centers[i, :] = x[np.argmax(density)]\n",
    "plt.scatter(centers[:, 0], centers[:, 1], s=500, marker='X')\n",
    "print(centers)\n",
    "plt.savefig('GMM_RS42_centers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing the random state, the standard deviation of data remains the same while the mean changes. The change in the mean impacts our Z score as well but since the change in our mean is minimal, this does not change anything. The cluster centres do change which is the expected observation however, the genral vicinity of them do not different from when random_state was set to 0.\n",
    "\n",
    "The cluster centroid are consistant with the K-means clustering and previuse gmm clusters. It remains to be seen wheteher it is consitant when all dimensions are present. As shown by 2d reduction in PCA, majority of the data is lost so it is expected for these clusters to not be accurate and not a good representation of different classes.\n",
    "\n",
    "Cell below calculates how accurate our predictions are compared to the true values within the dataset. \"accuracy()\" function takes 2 inputs, compares them and returns a percentage on how equivilant they are. Furthermore, we see how many different classes our predictions included that were true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, actual):\n",
    "    x = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == actual[i]:\n",
    "            x+=1\n",
    "    return print('accuracy percentage is:', (((x)/len(predictions))*100))\n",
    "\n",
    "accuracy(pred_labels, train_y_labels)\n",
    "\n",
    "correct_pred = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "for i in range(3000):\n",
    "        if pred_labels[i] == train_y_labels[i]:\n",
    "                    val = train_y_labels[i]\n",
    "                    correct_pred[val]+=1\n",
    "\n",
    "print(correct_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acurracy of the prediction are very low as predicted by our PCA analysis. Using all of the training data (60000), the acuraccy is only \"7.948333333333333\" percent or almost 8%. Moreover from the output, we observe that classes 3, 4, 8 and 10 were not at all predicted correctly while class 6 had the most correct predictions by a landscape although the predcition rate there was low as well. Recall from our PCA analysis the class associated with \"shirts\" were the most visible when we reduced our dimensions to 2d. So it should come as no surprise when class 6 which is shirts were predicted correctly the most. White shoes from snekaers class was the other closest visible however it was not predicted all but once within our prediction\n",
    "\n",
    "Overall the soft clustering on a 2d reduction does not output adequate predictions as with this reduction, majority of our information is lost. As mentioned before, reducing to 24 dimensions will ensure about 80% retention of information which is recommanded.\n",
    "\n",
    "We move on to ANN or Artificial Neural Networks.\n",
    "\n",
    "Firstly the logistic regression is fitted to our data which is a linear model for classification. The probabilities describing the possible outcomes of a classification are modeled using a logistic (sigmoid) function. We are trying to find the prediction accuracy of our train and test splits with this which are 87% and 84% respectively over 100 iterations.\n",
    "\n",
    "Training Neural networks is acomplished by learning the weights and biases in the hidden and output layers such that the network, outputs the correct optimal labels. Network's performance is defined by a loss function for which here a logistic regression is employed. The cost function is the sum of all the erroror logified, applied to all the training datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_nn = x_train_standard[:3000]\n",
    "x_test_nn = x_test_standard[:3000]\n",
    "\n",
    "test_y_nn = test_y_labels[:3000]\n",
    "train_y_nn = train_y_labels[:3000]\n",
    "\n",
    "\n",
    "logreg = LogisticRegression(max_iter=100)\n",
    "logreg.fit(x_train_standard, train_y_labels)\n",
    "print('Testing accuracy: ', (logreg.score(x_test_nn, test_y_nn)))\n",
    "print('Training accuracy: ', (logreg.score(x_train_nn, train_y_nn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above a logistic regression was fit to our training data and the accuracy of both testing and training data was measured. it is a linear model of classification that is modeled after the sigmoid function. On 3000 datapoints over 100 iterations, this model's accuracy against training data was 86.2% while it was 84.7% on testing data. The gap between the scores is not high suggesting that our model is likely not overfitting. It is worth mentioning that running this on all datapoints for over 2000 iteration failed to converge and yielded almost 87% accuracy on both testing and training datasets however that is very time consuming to compute and almost fried my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(64),\n",
    "                    activation='relu',\n",
    "                    solver='sgd',\n",
    "                    alpha=0,\n",
    "                    learning_rate_init=1e-3,\n",
    "                    max_iter=1800,\n",
    "                    n_iter_no_change=25,\n",
    "                    random_state=1)\n",
    "\n",
    "train = mlp.fit(x_train_nn, train_y_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(train.loss_curve_, c='orange')\n",
    "ax.set_title('orange-train, blue-test', c='red')\n",
    "ax.set_xlabel('Number of iterations', c='red')\n",
    "ax.set_ylabel('Loss', c='red')\n",
    "\n",
    "test = mlp.fit(x_test_nn, test_y_nn)\n",
    "\n",
    "ax.plot(test.loss_curve_, c='blue')\n",
    "plt.savefig('lossCurve_ANN64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the code cell above, ANN was applied using the multiple layer classifier from sklearn python library. The loss curve for both test and training data was plotted and it was observed that they were both extremely similar. This was expected due to logistic regression's score for both training and testing data being so close to each other. This ANN model, does not seem to be overfitting as the loss for both datasets are low. Also, the model's loss curves, do not climb after the initial dip which is a sign that the model is not overfitting. Furthurmore, our data is large and was standardized before applying the classifier which reduces overfitting. We move on to plotting a confusion matrix to understand our model's accuracy, as well as figuring out accuracy of our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing accuracy:', mlp.score(x_test_nn, test_y_nn))\n",
    "print('training accuracy:', mlp.score(x_train_nn, train_y_nn))\n",
    "\n",
    "plot_confusion_matrix(mlp, x_test_nn, test_y_nn)\n",
    "plt.savefig('ConfusionMatrix_ANN64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores, or the accuracy mean, of the model against testing data is 82.7% while it is 98.2% against training data. Our network model performs well on both datasets however the gap between the two datasets seems to be a bit high than ideal suggesting a slight overfitting. Furthermore a confusion matrix was plotted to understand our model's performance. From this we can see that our model against testing data made the most mistake when attempting to distinguish between classes 0(t-shirts/top) and 6(shirt), 4(coats) and 2(pullover), 6(shirt) and 2(pullover) and so on. It can be understood keeping in mind the similiarities between these classes however the model did very well with over 83% accuracy across all of the data points. Similar story was told when plotting the confusion matrix against our training datasets however there were more errors in the same classes.\n",
    "\n",
    "Our model's performance is dependant on our hyperparameters. Optimizing them will lead to better performance by our model and is a crucial part of machine learning. Ultimateley we want our Score to be as high as possible which means our model is nearer to perfect accuracy. Here we will use randomizedsearchCV to perform hyperparameters tuning, here we set the cross validation split to 3, each having 10 iterations for performance optimization. We will optimize initial learning rate and alphas. Alpha here refers to regularization term's strength which is devided by the sample size and added to the loss. This parameter controls the weighting of our regularization within cost funtion to better control overfitting. Initial learning rate \"m\" is another important parameter which is used in updating the weights by controling the step-size. For consistency we will keep every other Hyperparameter the same. Note that this takes a long time to carry out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates_init = np.logspace(-4, -2, 100)\n",
    "alphas = np.logspace(-3, -0, 100)\n",
    "\n",
    "hype_param = {'learning_rate_init':learning_rates_init, 'alpha':alphas}\n",
    "\n",
    "clf = RandomizedSearchCV(mlp, hype_param, cv=3, n_iter=10, verbose=2, scoring='accuracy', )\n",
    "\n",
    "clf.fit(x_train_nn, train_y_nn)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial learning rate of \" 0.00031992671377973844\" and alpha or regularisation of \"0.006579332246575682\" has been outputed. Below we apply a new neural network with these tweaked hyperparameters and see if the accuracy of our data changes. All other hyperparaneters stay unchanged. This NN, will have 64 layers and 1800 iteration over 3000 datasets just as the previous classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPClassifier(hidden_layer_sizes=(64),\n",
    "                    activation='relu',\n",
    "                    solver='sgd',\n",
    "                    alpha=0.006579332246575682,\n",
    "                    learning_rate_init=0.00031992671377973844,\n",
    "                    max_iter=1800,\n",
    "                    n_iter_no_change=25,\n",
    "                    random_state=1)\n",
    "\n",
    "train = nn.fit(x_train_nn, train_y_nn)\n",
    "\n",
    "print('Testing accuracy:', nn.score(x_test_nn, test_y_nn))\n",
    "print('training accuracy', nn.score(x_train_nn, train_y_nn))\n",
    "\n",
    "plot_confusion_matrix(nn, x_test_nn, test_y_nn)\n",
    "plt.savefig('con_matrix_tuned')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(train.loss_curve_, c='orange')\n",
    "ax.set_title('orange-test, blue-train', c='red')\n",
    "ax.set_xlabel('Number of iterations', c='red')\n",
    "ax.set_ylabel('Loss', c='red')\n",
    "test = nn.fit(x_test_nn, test_y_nn)\n",
    "ax.plot(test.loss_curve_, c='blue')\n",
    "plt.savefig('lossCurve_tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The changes done on these hyperparameters does slightly improve our accuracy against testing data. Our accuracy is 83.1% with the new tuned hyperparameters which is an improvement of 0.4%. Similar to our earlier neural network. Our loss curve was plotting to check for overfitting. The results are fairly similar and there is no sign of significant overfitting.\n",
    "\n",
    "For furthere changes, here a new neural network is model with more layers and a different solver. Previously, we have used 64 hidden layers and the 'sgd' solver. SGD or stochastic gradient descent, computes an approximation rather than the true value of gradiet in each step which leads to noisier gradient decebt. It is mostly used because of simplicity and how fast it runs since it is linear. Adams's solver, have faster run time and need lesser parameters for optimization. For hidden layers, increasing them can lead to higher accuracy however it can be very time consuming and expensive to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ann = MLPClassifier(hidden_layer_sizes=(100),\n",
    "                    activation='relu',\n",
    "                    solver='adam',\n",
    "                    alpha=0.006579332246575682,\n",
    "                    learning_rate_init=0.00031992671377973844,\n",
    "                    max_iter=1800,\n",
    "                    n_iter_no_change=25,\n",
    "                    random_state=1)\n",
    "\n",
    "train = Ann.fit(x_train_nn, train_y_nn)\n",
    "print('test accuracy:', Ann.score(x_test_nn, test_y_nn))\n",
    "print('train accuracy:', Ann.score(x_train_nn, train_y_nn))\n",
    "\n",
    "plot_confusion_matrix(Ann, x_test_nn, test_y_nn)\n",
    "plt.savefig('tuned_100_confusionMat')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(train.loss_curve_, c='orange')\n",
    "ax.set_title('orange-test, blue-train', c='red')\n",
    "ax.set_xlabel('Number of iterations', c='red')\n",
    "ax.set_ylabel('Loss', c='red')\n",
    "test = Ann.fit(x_test_nn, test_y_nn)\n",
    "ax.plot(test.loss_curve_, c='blue')\n",
    "plt.savefig('lossCurve_tuned_100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two changes, have made our accuracy againts testing data go up to 83.2% which is 0.1% improvement. Against training data we finally achieve 100% accuracy with the adam solver and more hidden layers. Our loss curve here for both training and testing data set changes slightly as it has more bumps through the decending gradient. Our model might be overfitting slightly more compared to other curves however it is insignificant.\n",
    "\n",
    "For furthure tuning, we employ a gridsearchCV for tuning. For a given model, in our case a neural network, Grid Search is a cross-validation technique used for finding optimal hyperparameters when these are in a given grid and predictions are made. Layers will be 100 as we dont want to risk overfitting, while activation methods tested, will be 'softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', and 'linear'. Activation method, is responsible for deciding whether a neuron will be activated and its input is important within the neuron network. The previously tuned parameter, initial learning rate and regularisation will stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'hidden_layer_sizes': [100],\n",
    " 'activation': ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear'], \n",
    " 'solver': ['adam', 'sgd'],\n",
    " 'alpha': [0.00031992671377973844],\n",
    " 'learning_rate_init': [0.00031992671377973844],\n",
    " 'max_iter':[1800],\n",
    " 'n_iter_no_change':[25],\n",
    " 'random_state':[1]}\n",
    "gr = GridSearchCV(estimator=Ann, param_grid=parameters)\n",
    "gr_out = gr.fit(x_train_nn, train_y_nn)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (gr_out.best_score_, gr_out.best_params_))\n",
    "params = gr_out.cv_results_['params']\n",
    "for param in zip(params):\n",
    "    print(gr_out.score(x_test_nn, test_y_nn), param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the grid search, there are no furthure improvements to our accuracy. The activation parameter did not improve our performance the adam solver was proven to be the best. 'ReLU' or rectified linear activation will remain our parameter. This activation method only outputs if input is positive and output zero if its negative which impacts our performance and makes it faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM\n",
    "Support Vector Machines (SVM) are nonparametric model where the number of parameters are not fixed and increase appropriate to the size of our data. Function below is available from 'https://github.com/rasbt/mlxtend/blob/master/mlxtend/plotting/learning_curves.py' and is used for plotting a learning curve for our fitted SVM. This function plots misclassification errors (performance) against training set's size. The size of both our testing and training dataset will be 5000, the regularisation parameter \"c\" will be 1 (defult) (same as ANN classifier) and kernel used will be the default 'rbf'.\n",
    "\n",
    "Leaning curves shows both validation and traning scores as the number of our samples increase. It is a tool for estimating whether we benefit from adding more training data and how much our SVM is effected by variance or bias error. Two different function are utilised for visualising the learning curve, one function shows errors (plot_learning_curves) while the other is plotted using the accuracy of the model (plt_learn_curve_acc), both against training size.\n",
    "\n",
    "For validation curve, we can see from it whether our model is overfitting or underfitting. Scores for both training and testing datapoints are calculated and plotted against a range of regularisation factors \"C\". High training scores paired with low validation score, suggests the SVM is overfitting. For this, function (plt_valid_curve) is introduced taking in an estimator (SVM in this case) and training and target samples, to plot validation curve of regularisation against accuracy of the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(X_train, y_train, X_test, y_test, clf, train_marker=\"o\", test_marker=\"^\", scoring=\"misclassification error\", suppress_plot=False, print_model=True, title_fontsize=12,\n",
    "    style=\"default\", legend_loc=\"best\"):\n",
    "    if scoring != \"misclassification error\":\n",
    "        from sklearn import metrics\n",
    "        scoring_func = {\n",
    "            \"accuracy\": metrics.accuracy_score,\n",
    "            \"average_precision\": metrics.average_precision_score,\n",
    "            \"f1\": metrics.f1_score,\n",
    "            \"f1_micro\": metrics.f1_score,\n",
    "            \"f1_macro\": metrics.f1_score,\n",
    "            \"f1_weighted\": metrics.f1_score,\n",
    "            \"f1_samples\": metrics.f1_score,\n",
    "            \"log_loss\": metrics.log_loss,\n",
    "            \"precision\": metrics.precision_score,\n",
    "            \"recall\": metrics.recall_score,\n",
    "            \"roc_auc\": metrics.roc_auc_score,\n",
    "            \"adjusted_rand_score\": metrics.adjusted_rand_score,\n",
    "            \"mean_absolute_error\": metrics.mean_absolute_error,\n",
    "            \"mean_squared_error\": metrics.mean_squared_error,\n",
    "            \"median_absolute_error\": metrics.median_absolute_error,\n",
    "            \"r2\": metrics.r2_score,\n",
    "        }\n",
    "\n",
    "        if scoring not in scoring_func.keys():\n",
    "            raise AttributeError(\"scoring must be in\", scoring_func.keys())\n",
    "\n",
    "    else:\n",
    "\n",
    "        def misclf_err(y_predict, y):\n",
    "            return (y_predict != y).sum() / float(len(y))\n",
    "\n",
    "        scoring_func = {\"misclassification error\": misclf_err}\n",
    "\n",
    "    training_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    rng = [int(i) for i in np.linspace(0, X_train.shape[0], 11)][1:]\n",
    "    for r in rng:\n",
    "        model = clf.fit(X_train[:r], y_train[:r])\n",
    "\n",
    "        y_train_predict = clf.predict(X_train[:r])\n",
    "        y_test_predict = clf.predict(X_test)\n",
    "\n",
    "        train_misclf = scoring_func[scoring](y_train[:r], y_train_predict)\n",
    "        training_errors.append(train_misclf)\n",
    "\n",
    "        test_misclf = scoring_func[scoring](y_test, y_test_predict)\n",
    "        test_errors.append(test_misclf)\n",
    "\n",
    "    if not suppress_plot:\n",
    "        with plt.style.context(style):\n",
    "            plt.plot(\n",
    "                np.arange(10, 101, 10),\n",
    "                training_errors,\n",
    "                label=\"training set\",\n",
    "                marker=train_marker,\n",
    "            )\n",
    "            plt.plot(\n",
    "                np.arange(10, 101, 10),\n",
    "                test_errors,\n",
    "                label=\"test set\",\n",
    "                marker=test_marker,\n",
    "            )\n",
    "            plt.xlabel(\"Training set size in percent\", c='red')\n",
    "\n",
    "    if not suppress_plot:\n",
    "        with plt.style.context(style):\n",
    "            plt.ylabel(\"Performance ({})\".format(scoring), c='red')\n",
    "            if print_model:\n",
    "                plt.title(\n",
    "                    \"Learning Curves\\n\\n{}\\n\".format(model), fontsize=title_fontsize\n",
    "                )\n",
    "            plt.legend(loc=legend_loc, numpoints=1)\n",
    "            plt.xlim([0, 110])\n",
    "            max_y = max(max(test_errors), max(training_errors))\n",
    "            min_y = min(min(test_errors), min(training_errors))\n",
    "            plt.ylim([min_y - min_y * 0.15, max_y + max_y * 0.15])\n",
    "    errors = (training_errors, test_errors)\n",
    "    return errors\n",
    "\n",
    "\n",
    "def plt_learn_curve_acc(svm, x, y):\n",
    "    train_sizes, train_scores_svm, test_scores_svm = learning_curve(svm, x, y, scoring=\"accuracy\")\n",
    "    plt.plot(train_sizes, -test_scores_svm.mean(1), color=\"orange\")\n",
    "    plt.plot(train_sizes, -train_scores_svm.mean(1), color=\"blue\")\n",
    "    plt.xlabel(\"Train size\", c='red')\n",
    "    plt.ylabel(\"accuracy\", c='red')\n",
    "    plt.title(\"Learning curves\", c='red')\n",
    "    plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html#sphx-glr-auto-examples-model-selection-plot-validation-curve-py\n",
    "\n",
    "def plt_valid_curve(svm, x_train_svm, train_y_svm):\n",
    "    param_range = np.logspace(-5, 3, 10)\n",
    "    train_scores, test_scores = validation_curve(\n",
    "    svm,\n",
    "    x_train_svm,\n",
    "    train_y_svm,\n",
    "    param_name=\"C\",\n",
    "    param_range=param_range,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=2,\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.title(\"Validation Curve with SVM\", c='red')\n",
    "    plt.xlabel(\"regularisation\", c='red')\n",
    "    plt.ylabel(\"Score\", c='red')\n",
    "    plt.ylim(0.0, 1.1)\n",
    "    lw = 2\n",
    "    plt.semilogx(\n",
    "        param_range, train_scores_mean, label=\"Training score\", color=\"darkorange\", lw=lw\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        param_range,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.2,\n",
    "        color=\"darkorange\",\n",
    "        lw=lw,\n",
    "    )\n",
    "    plt.semilogx(\n",
    "        param_range, test_scores_mean, label=\"Cross-validation score\", color=\"navy\", lw=lw\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        param_range,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.2,\n",
    "        color=\"navy\",\n",
    "        lw=lw,\n",
    "    )\n",
    "    plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_svm = x_train_standard[:5000]\n",
    "x_test_svm = x_test_standard[:5000]\n",
    "train_y_svm = train_y_labels[:5000]\n",
    "test_y_svm = test_y_labels[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C=1, kernel='rbf')\n",
    "svm.fit(x_train_svm, train_y_svm)\n",
    "print('SVM classifier accuracy againts training:', svm.score(x_train_svm, train_y_svm))\n",
    "print('SVM classifier accuracy against testing:', svm.score(x_test_svm, test_y_svm))\n",
    "\n",
    "plt_valid_curve(svm, x_train_svm, train_y_svm)\n",
    "plt.savefig('validCurve_SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning curve against error\n",
    "plot_learning_curves(x_train_svm, train_y_svm, x_test_svm, test_y_svm, svm)\n",
    "plt.show()\n",
    "plt.savefig('LeanCurve_SVM_error')\n",
    "\n",
    "#learning curve against accuracy\n",
    "plt_learn_curve_acc(svm, x_train_svm, train_y_svm)\n",
    "plt.savefig('learnCurve_SVM_acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM model's score against our testing dataset is 84.1% while it is 90.0% against training set.\n",
    "From the plot ouputed, the SVM model could not benefit from additional training data as the gradient for testing rapidly decreases as we reach all 5000 training size. So the testset error is not likely to decrease furthure than 12%. As we reach maximum training size, the error gap between testing and training data is 6% which is a not too large, suggesting the model does well.\n",
    "\n",
    "Furthermore, with small datapoints, the gap between training and test set's performance is high. This can be addressed by increasing the regularisation value and the number of training samples, in which here we increased the input size. As the number of data increases the error for both testing and training dataset decrease showing that our model is performing well. Moreover, the accuracy of the model increases as the number of data points increase, the lowest value with 500 datapoints is around 76% while the highest with 4000 datapoints is about 85%.\n",
    "\n",
    "From the validation curve, it is observed that both training score and our cross-validation score are very similar. suggesting that our model is working fine and is not overfitting or underfitting. In general, for small sample data, training score for SVM is bigger than validation score. This can be addressed by adding more sample data. As the regularisation term increases with our sample size, the score for both training data and validation increase which means our model is performing well.\n",
    "\n",
    "In the code cell below, we use a grid search to tune our hyperparameters and extract the best to carry forward. The regularisation term \"C\" will ave 100 values between 0-20, and different kernels tested are 'sigmoid', 'rbf' and 'linear' are fitted. This will be 300 different fits with 3 cross validations leaving us with 900 fits for 5000 datapoints. The kernel specifies the type of algorithm to be used. \"Linear\" is the most basic and the fastest type of kernel and works best when there are a lot of features present like in fashion dataset. For non-linear data, rbf or Gaussian radial basis function is preffered while sigmoid kernels are preffered for neural networks. Gamma or the kernel coefficent is left as defult value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': np.linspace(0,20,100), 'kernel': ['rbf', 'linear', 'sigmoid']} \n",
    "  \n",
    "grid = GridSearchCV(svm, param_grid, verbose = 2, cv=3)\n",
    "\n",
    "grid.fit(x_train_svm, train_y_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C=0.20202020202020202, gamma=0.01, kernel='linear')\n",
    "svm.fit(x_train_svm, train_y_svm)\n",
    "\n",
    "print(svm.score(x_train_svm, train_y_svm))\n",
    "print(svm.score(x_test_svm, test_y_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this search, our best regularisation factor is \"0.20202020202020202\" and \"linear\" as the best kernel. These hyperparameters improve our training accuracy to 95% from 90% (using rbf kernel, and c=1) while they become less accurate against testing data with 82.2% accuracy from 84.1% suggesting a very slight overfitting.\n",
    "\n",
    "Below the kernel parameter and how sensitive SVM is to them is tested by plotting learning curve and printing their accuracy. The tuned regularisation parameter (0.20202020202020202) and gamma 0.01 for all kernel types will be utilised. Furtheremore, validation curve for each kernel type is plotted against different regularisation values for better analysis and visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_linear = SVC(C=0.20202020202020202, gamma=0.01, kernel='linear')\n",
    "svm_sigmoid = SVC(C=0.20202020202020202, gamma=0.01, kernel='sigmoid')\n",
    "svm_rbf = SVC(C=0.20202020202020202, gamma=0.01, kernel='rbf')\n",
    "\n",
    "svm_linear.fit(x_train_svm,train_y_svm)\n",
    "svm_rbf.fit(x_train_svm,train_y_svm)\n",
    "svm_sigmoid.fit(x_train_svm,train_y_svm)\n",
    "\n",
    "plt_learn_curve_acc(svm_rbf, x_train_svm, train_y_svm)\n",
    "plt.savefig('RBFlearnCurve_acc_tuned_SVM')\n",
    "plt.show()\n",
    "\n",
    "plt_learn_curve_acc(svm_sigmoid, x_train_svm, train_y_svm)\n",
    "plt.savefig('Sig_learCurve_tuned_SVM')\n",
    "plt.show()\n",
    "\n",
    "plt_learn_curve_acc(svm_linear, x_train_svm, train_y_svm)\n",
    "plt.savefig('linear_learnCurve_tuned_svm')\n",
    "plt.show()\n",
    "\n",
    "print('rbf accuracy', svm_rbf.score(x_test_svm, test_y_svm))\n",
    "print('sigmoid accuracy', svm_sigmoid.score(x_test_svm, test_y_svm))\n",
    "print('linear accuracy', svm_linear.score(x_test_svm, test_y_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_valid_curve(svm_linear, x_train_svm, train_y_svm)\n",
    "plt.savefig('validCurve_SVM_Linear')\n",
    "\n",
    "plt_valid_curve(svm_rbf, x_train_svm, train_y_svm)\n",
    "plt.savefig('ValidCurve_svm_RBF')\n",
    "\n",
    "plt_valid_curve(svm_sigmoid, x_train_svm, train_y_svm)\n",
    "plt.savefig('validCurve_svm_Sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots and final accuracy score, we can conclude that the best kernel is the linear kernel since it has a high accuracy score and better learning curve. The sigmoid kernel is the worst performing out of all since it has a very low score of 41.5% and loses accurcy as we increase our input samples. This kernel improves in accuracy with small sample size and maxes at 60% with about 1200 datapoints but beyond that rapidley declines in score. \"rbf\" kernel does well with both score (79.6%) and learning curve however it is outperformed by linear model (82.2%) by a margain of 12%.\n",
    "\n",
    "The validation curve for both rbf and linear are the best suggesting that our model is performing very well with a little gap between testing and training scores. Before we used all positive values of C for tunining (ranging from 0-20) however the validation curve shows that this kernel can be optimised by having a regularisation factor between \"10^-4 - 0\". It is worth mentioning that accuracy of sigmoid kernel does not go higher than 60%, with a clear seperation between validation and training curve suggesting overfitting, which is still the third best kernel out of the tested three.\n",
    "\n",
    "In conclusion, the best kernel is the \"linear\" and for it the regularisation factor \"c\" of \"0.20202020202020202\" gives us the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM vs ANN:\n",
    "\n",
    "Both Support Vector Machines and Artificial Neural Networks have much shared features. These two classifiers both have comparable accuracy and perform well on the given data. These parameteric classifiers both can accomidate non-linear data with usage of Kernels in SVM and activation in ANN. To the contrary, ANN has the edge over SVM due to higher accuracy and overall better performance. Given enough data, and training, ANN usually outperforms SVM and it was seen in our case. Given we have enough computational power, if the input samples and number of layers within the ANN were to increase ANN will far out perform SVM. This is due to additional complexity ANN offers with hidden network layers.\n",
    "\n",
    "However, we dont always have infinite computational power and thats where using SVM is preferred. SVMs are in general very fast to train, since they only use a subset of data and rely on support vectors for decision boundaries. Large number of samples for ANN will required long time commitments making them a very expensive classifying method. This is furthure shown below with a bar chart, comparing time taken to train 5000 samples for both classifiers. SVM took only 1.3 seconds while ANN took 39.2 seconds to train and it is expected to increase even furthure as the trainning size increasses. Note that for both classifiers, the tuned hyperparameters were used. This shows us that changing our parameters and re-training a NN is far more expensive than using SVM.\n",
    "\n",
    "In conclusion, ANN performs better than SVM since it has higher accuracy against testing data however they are time consuming to train. Depending on available resources, time and accuracy needed, we can use either classifier to train our data. In this case, using only a subset of data available to us mean time and computational power were not an issue therefore using ANN is the preferred method of classifying. This is furthure confirmed by looking at our accuracy scores, with our tuned ANN with observe 83.2% accuracy while it is 82.2% accuracy for SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple bar chart comparing time taken to train 5000 samples wtih SVM and ANN\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "classifiers = ['SVM', 'ANN']\n",
    "time = [1.3, 39.2]\n",
    "\n",
    "x_pos = [i for i, _ in enumerate(classifiers)]\n",
    "colours = ['green', 'red']\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.bar(x_pos, time, color=colours)\n",
    "plt.xlabel(\"classifier\")\n",
    "plt.ylabel(\"Time\")\n",
    "plt.title(\"ANN vs SVM time\")\n",
    "\n",
    "plt.xticks(x_pos, classifiers)\n",
    "\n",
    "plt.savefig('SVM_VS_ANN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3:Bayesian linReg:\n",
    "\n",
    "In the code cell below, california housing dataset is loaded and scattered with respect to lognitude and latitude values. Furthuremore the median house values are grouped together and represented in different colours. The side bar next to the scatter plot showcases different groups and as numbers get higher so do the median house values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali_house = fetch_california_housing(as_frame=True)\n",
    "x_cali_df, y_cali_df = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "x_cali, y_cali = fetch_california_housing(return_X_y=True)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.scatterplot(data=cali_house.frame, x=\"Longitude\", y=\"Latitude\", size=\"MedHouseVal\", hue=\"MedHouseVal\", palette=\"viridis\", alpha=0.5)\n",
    "plt.legend(title=\"MedHouseVal\", bbox_to_anchor=(1.05, 0.95), loc=\"upper left\")\n",
    "\n",
    "_ = plt.title(\"Median house value depending on their spatial location\")\n",
    "plt.savefig('cali_houses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.1:\n",
    "\n",
    "From the scatter plot of all datapoints we observe that as we get closer to the eastern side of the map, the median house values increase. So this mean lower longitude values paired latitude values lower than approximately 39, will have the higher median values. This can be understood since california is a warmer climate location and the afformentioned locations are near or next to the beach and sea side. From this, an initial theory can be concluded that as the houses get closer to the sea side, the higher the house values will be. The centeral side of the map has some high valued houses as compared to more out skirts of the plot. This can be due to neighbourhood the houses are in however, the sea side remain the highest valued houses in general.\n",
    "\n",
    "2.3.2:\n",
    "\n",
    "Data transformation is a technique to make the data more in-line with others and/or have it be more organised. They are also simpler for computers to use and make it easier to do various analysis such as plotting. Furthermore, if the data is validated and formated, it leads to less errors such as duplicates, null values and the overall effeciency of different analysis increase. Since we are dealing with linear regression, it is best if we transform our dataset. This is done to make sure we can achieve either linearality, normality or a stable variance. In this case, linearality is the objective so transforming the data is a common practice. \n",
    "\n",
    "Decision Making is having a conclusion by exploring all different outcomes. It is a complex task that involves analysing different data collected from various sources with different certainty levels. Having clean data can improve our effeciency and give us higher quality of information for analysis. The purpose here, is to have decision making power so poorly collected datasets often cause models to incorectly represent information, thereby reducing their decision-making powers.\n",
    "\n",
    "Here, all different features are put into a single dataframe without \"Longitude\" and \"Latitude\" values. Two different dataframes are created one with target values or \"MedHouseVal\" merged and another without. the dataframe without the merged target values and two dropped features, is scaled and is of shape (20640, 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping longitude and latitude values to focus on other features\n",
    "columns_drop = [\"Longitude\", \"Latitude\"]\n",
    "gulag_ed_cali = cali_house.frame.drop(columns=columns_drop)\n",
    "gulag_ed_cali[\"MedHouseVal\"] = pd.qcut(gulag_ed_cali[\"MedHouseVal\"], 6, retbins=False)\n",
    "gulag_ed_cali[\"MedHouseVal\"] = gulag_ed_cali[\"MedHouseVal\"].apply(lambda x: x.mid) #with MedHouseVal\n",
    "\n",
    "columns_drop = [\"Longitude\", \"Latitude\", \"MedHouseVal\"]\n",
    "california_x_df = cali_house.frame.drop(columns=columns_drop) #without MedHouseVal\n",
    "\n",
    "scalar = StandardScaler()\n",
    "x_scaled_cali = scalar.fit_transform(california_x_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(x, y):\n",
    "    mean = []\n",
    "    for i in range(len(x)):\n",
    "        all_vals = np.mean(x[i,y])\n",
    "        mean.append(all_vals)\n",
    "    return mean\n",
    "\n",
    "def separate(x, y):\n",
    "    spared = []\n",
    "    for i in range(len(x)):\n",
    "        spared.append(x[i,y])\n",
    "    return spared\n",
    "\n",
    "medinc = unique(x_scaled_cali, 0)\n",
    "houseAge = unique(x_scaled_cali, 1)\n",
    "aveRooms = unique(x_scaled_cali, 2)\n",
    "aveBedrms = unique(x_scaled_cali, 3)\n",
    "aveOccup = unique(x_scaled_cali, 4)\n",
    "pop = unique(x_scaled_cali, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.3: NORMAL distribution\n",
    "\n",
    "Using \"PyMc\", an approximate distribution with our scaled data is produced. In the code cell above some pre processing has taken place to prepere the data for creating the a normal distribution model. Each feature's values, are separated into a unique array. These separated data are used then used to define 6 different priors using their mean and standard deviation. A uniform standard deviation is defined with upper max of 20 and lower min of 0. Furthuremore all priors are used to have a estimated output for calculating a likelihood for all our data. Sample are taken from our priors and trace is produced for calculating the posterior distribution of this model. Note that the sample draws 16512 samples, or the length of our train split, there are ten cores and two chains. Sampling method chosen here is \"no U turn sampler\". This is an adaptive sampler that uses distance travelled against the density of target's curve. It is a recurssive algorithm that does not require a set number of stemps and is simple to utilise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    medInc_p = pm.Normal('medInc_p', mu=np.mean(medinc), sigma=np.std(medinc))\n",
    "    houseAge_p = pm.Normal('houseAge_p', mu=np.mean(houseAge), sigma=np.std(houseAge))\n",
    "    aveRooms_p = pm.Normal('aveRooms_p', mu=np.mean(aveRooms), sigma=np.std(aveRooms))\n",
    "    aveBedrms_p = pm.Normal('aveBedrms_p', mu=np.mean(aveBedrms), sigma=np.std(aveBedrms))\n",
    "    aveOccup_p = pm.Normal('aveOccup_p', mu=np.mean(aveOccup), sigma=np.std(aveOccup))\n",
    "    pop_p = pm.Normal('pop_p', mu=np.mean(pop), sigma=np.std(pop))\n",
    "    sig = pm.Uniform('std', lower=0, upper=20)\n",
    "    mu = (medInc_p * separate(x_scaled_cali, 0)) + (houseAge_p * separate(x_scaled_cali, 1)) + (aveRooms_p * separate(x_scaled_cali, 2)) + (aveBedrms_p * separate(x_scaled_cali, 3)) + (aveOccup_p * separate(x_scaled_cali, 4)) + (pop_p * separate(x_scaled_cali, 5))\n",
    "    likelihood = pm.Normal('y', observed=y_cali[0], mu=mu, sigma=sig)\n",
    "    sample = pm.NUTS()\n",
    "    trace = pm.sample(len(x_scaled_cali), sample, progressbar=True, cores=10, chains=2)\n",
    "\n",
    "az.plot_posterior(trace)\n",
    "plt.savefig('posterior_trace.png')\n",
    "\n",
    "z_score = np.mean(scipy.stats.zscore(abs(x_scaled_cali)))\n",
    "p_value = scipy.stats.norm.sf(abs(z_score))\n",
    "print('p value is:', p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we see posterior distribution given our priors and likelihood. The mean for each prior is near 0 except \"AveRooms\" and \"AveBedrms\" whcih both are -0.001, and standard deviation: ['0.045','0.034','0.085','0.080','0.034','0.032']. For all of our data the man is '4.58' with standard deviation of '0.022'.\n",
    "\n",
    "2.3.4:\n",
    "\n",
    "These distributions summerise uncertain quantitues in bayesian analysis, and provides new information from our data. HDI or Highest density interval, can demosntrate where the most credible points of a distribution are. Here the HDI is 94% meaning thats where all the information is. The R-hat value is an indicator that signals if we have achieved stationarity with our chains. Here, 2 chains are used and the r-hat value for all is \"1.0\". A good value for this is between \"1.2-0.9\" there fore these distributions are a decent approximations since these outputs are evidence that all chains have converged. The P value for all our scaled data set is 0.5. This value suggests that our values are statisticaly significant. The \"ess_bulk\" value is used to understand sampling effeciency for the bulk of posterior. For features: \"HouseAge\", \"AveOccup\" and \"Population\" this value has a large gap with other features, meaning that we will need higher sampling points in order to have more effective rank normalisation in split chains and there is not much that can be extracted from them as features relative to other features. The monty carlo standard error mean for each prior is near zero with standard deviation being zero at all time. This shows that the accuracy of the chains are high and the model is performing well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_50 = x_scaled_cali[:50]\n",
    "x_500 = x_scaled_cali[:500]\n",
    "\n",
    "with pm.Model() as tree2:\n",
    "    medInc_p = pm.Normal('medInc_p', mu=np.mean(medinc), sigma=np.std(medinc))\n",
    "    houseAge_p = pm.Normal('houseAge_p', mu=np.mean(houseAge), sigma=np.std(houseAge))\n",
    "    aveRooms_p = pm.Normal('aveRooms_p', mu=np.mean(aveRooms), sigma=np.std(aveRooms))\n",
    "    aveBedrms_p = pm.Normal('aveBedrms_p', mu=np.mean(aveBedrms), sigma=np.std(aveBedrms))\n",
    "    aveOccup_p = pm.Normal('aveOccup_p', mu=np.mean(aveOccup), sigma=np.std(aveOccup))\n",
    "    pop_p = pm.Normal('pop_p', mu=np.mean(pop), sigma=np.std(pop))\n",
    "    sig = pm.Uniform('std', lower=0, upper=20)\n",
    "    mu = (medInc_p * separate(x_50, 0)) + (houseAge_p * separate(x_50, 1)) + (aveRooms_p * separate(x_50, 2)) + (aveBedrms_p * separate(x_50, 3)) + (aveOccup_p * separate(x_50, 4)) + (pop_p * separate(x_50, 5))\n",
    "    likelihood = pm.Normal('y', observed=y_cali[0], mu=mu, sigma=sig)\n",
    "    sample = pm.NUTS()\n",
    "    trace_50 = pm.sample(len(x_50), sample, progressbar=True, cores=10, chains=2)\n",
    "\n",
    "az.plot_posterior(trace_50)\n",
    "plt.savefig('posterior_trace_50.png')\n",
    "\n",
    "z_score = np.mean(scipy.stats.zscore(abs(x_50)))\n",
    "p_value = scipy.stats.norm.sf(abs(z_score))\n",
    "print('p value is for 50 values:', p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    medInc_p = pm.Normal('medInc_p', mu=np.mean(medinc), sigma=np.std(medinc))\n",
    "    houseAge_p = pm.Normal('houseAge_p', mu=np.mean(houseAge), sigma=np.std(houseAge))\n",
    "    aveRooms_p = pm.Normal('aveRooms_p', mu=np.mean(aveRooms), sigma=np.std(aveRooms))\n",
    "    aveBedrms_p = pm.Normal('aveBedrms_p', mu=np.mean(aveBedrms), sigma=np.std(aveBedrms))\n",
    "    aveOccup_p = pm.Normal('aveOccup_p', mu=np.mean(aveOccup), sigma=np.std(aveOccup))\n",
    "    pop_p = pm.Normal('pop_p', mu=np.mean(pop), sigma=np.std(pop))\n",
    "    sig = pm.Uniform('std', lower=0, upper=20)\n",
    "    mu = (medInc_p * separate(x_500, 0)) + (houseAge_p * separate(x_500, 1)) + (aveRooms_p * separate(x_500, 2)) + (aveBedrms_p * separate(x_500, 3)) + (aveOccup_p * separate(x_500, 4)) + (pop_p * separate(x_500, 5))\n",
    "    likelihood = pm.Normal('y', observed=y_cali[0], mu=mu, sigma=sig)\n",
    "    sample = pm.NUTS()\n",
    "    trace_500 = pm.sample(len(x_500), sample, progressbar=True, cores=10, chains=2)\n",
    "\n",
    "az.plot_posterior(trace_500)\n",
    "plt.savefig('posterior_trace_500.png')\n",
    "\n",
    "z_score = np.mean(scipy.stats.zscore(abs(x_500)))\n",
    "p_value = scipy.stats.norm.sf(abs(z_score))\n",
    "print('p value is for 500 values:', p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace_500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.5:\n",
    "\n",
    "For all 3 different sample distributions, They hold 94% HDI, meaning the majority of information are still within the same vicinity. The R-har value for 50 samples, range between \"1.00-1.05\" suggesting although each chain did not fully replicate the posteriors, they did converge to an equiliberium. Furthure the P-value for all 3 samples are 0.5 so nothing changes there, still suggesting significance. The posterior distribution for both 50 and 500 samples look different and less certain. This is due to the gradient of the slope for each prior suddenly and constantly changing. This is far more visible with 50 sample points and less so with 500. Reasoning for it is due to available sample points as with the full dataset, the distributions look more certain. Furthermore, \"ess_bulk\" and \"ess_tail\" values for 50 datapoints indicate that using this small sample size is not ideal. The same can be observed with 500 sample points. Finally, the monte carlo standard error for both 50 and 500 sample size are relative to full dataset high, meaning the model's performance is not ideal. These observations were expected as the accuracy of a model would vary based on how much evidence is available for initiating modeling. Therefore, using higher sample sizes is better. Overall, \"MedInc\", \"AveRooms\" and \"AveBedrms\" have been the most contributers to our model's predictions for house prices in california."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.1.1:\n",
    "\n",
    "CART or classification and regression tree is a predictive model, which predicts different variable's outcome values, with respect to other matters. It's algorithm starts by taking the best split point for each input, then takes the splits and furthere finds another split. Then split a chosen input based to the new best split point. This process continues until there is no furthure best split is possible.\n",
    "The first split is accomplished by utilising a threshold value of an attribute, then the nodes are split into sub-nodes. The sub-nodes are furthure split based on two diffferent variables, best attribute and threshold value.\n",
    "\n",
    "Here \"DecisionTreeRegressor\" is used since it can solve both classification and linear regression problems and also, because the data is continuous. All hyperparameters are set to default and random state is \"0\". Dataset is also split 80% train and 20% test with random state set to 66."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cali, x_test_cali, y_train_cali, y_test_cali = train_test_split(x_scaled_cali, y_cali, random_state=66, test_size=0.2)\n",
    "\n",
    "tree = DecisionTreeRegressor(random_state=0)\n",
    "tree.fit(x_train_cali, y_train_cali)\n",
    "tree.predict(x_train_cali)\n",
    "score = tree.score(x_test_cali, y_test_cali)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score for this model against the test values, is 42.4%. We can furthure improve this accuracy by tuning our hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.1.2: Hyperparameter Tuning\n",
    "\n",
    "Parameters criterion to measure quality of splits, splitter for splitting method used, max_depth for tree's maximum depth, min_samples_split for minimum number of samples required to split an internal node, min_samples_leaf number to be at a leaf node, and min_weight_fraction_leaf are tuned using grid search. For optimisation, only 4000 datapoints were used in the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_cali[:4000]\n",
    "y_train = y_train_cali[:4000]\n",
    "min_sample_leaf = [1,2,3,4,5]\n",
    "min_weight_fraction_leaf = [0.1,0.2,0.3,0.4]\n",
    "min_samples_split = [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "\n",
    "parameters = {'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    " 'splitter': ['best', 'random'], \n",
    " 'max_depth': np.linspace(0,100,5),\n",
    " 'min_samples_split': min_samples_split,\n",
    " 'min_samples_leaf': min_sample_leaf,\n",
    " 'min_weight_fraction_leaf': min_weight_fraction_leaf}\n",
    "\n",
    "gr = GridSearchCV(estimator=tree, param_grid=parameters, cv=5)\n",
    "gr_out = gr.fit(x_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (gr_out.best_score_, gr_out.best_params_))\n",
    "params = gr_out.cv_results_['params']\n",
    "\n",
    "#Best: 0.498685 using {'criterion': 'absolute_error', 'max_depth': 25.0, 'min_samples_leaf': 1, 'min_samples_split': 0.2, 'min_weight_fraction_leaf': 0.1, 'splitter': 'best'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, best accouracy was 50% with parameters, absolute_error as cirterion, 25 as max_depth, 1 as min_samples_leaf, 0.2 as min_samples_split and best as splitter. To understand what hyperparameter this model is sensetive towards, the tuned ones will be passed while a single hyperparameter will be set to defualt. Furthuremore, the accuracy of each model will be assesed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_fried = DecisionTreeRegressor(criterion='friedman_mse', splitter='best', max_depth=25, min_samples_leaf=0.2, min_weight_fraction_leaf=0.1)\n",
    "tree_fried.fit(x_train_cali, y_train_cali)\n",
    "tree_fried.predict(x_train_cali)\n",
    "score_fried = tree_fried.score(x_test_cali, y_test_cali)\n",
    "print(\"friedman score:\", score_fried)\n",
    "\n",
    "tree_poss = DecisionTreeRegressor(criterion='poisson', splitter='best', max_depth=25, min_samples_leaf=0.2, min_weight_fraction_leaf=0.1)\n",
    "tree_poss.fit(x_train_cali, y_train_cali)\n",
    "tree_poss.predict(x_train_cali)\n",
    "score_poss = tree_poss.score(x_test_cali, y_test_cali)\n",
    "print(\"poisson score:\", score_poss)\n",
    "\n",
    "tree_square = DecisionTreeRegressor(criterion='squared_error', splitter='best', max_depth=25, min_samples_leaf=0.2, min_weight_fraction_leaf=0.1)\n",
    "tree_square.fit(x_train_cali, y_train_cali)\n",
    "tree_square.predict(x_train_cali)\n",
    "score_square = tree_square.score(x_test_cali, y_test_cali)\n",
    "print(\"squared_error score:\", score_square)\n",
    "\n",
    "tree_random = DecisionTreeRegressor(criterion='absolute_error', splitter='random', max_depth=25, min_samples_leaf=0.2, min_weight_fraction_leaf=0.1)\n",
    "tree_random.fit(x_train_cali, y_train_cali)\n",
    "tree_random.predict(x_train_cali)\n",
    "score_random = tree_random.score(x_test_cali, y_test_cali)\n",
    "print(\"random splitter score:\", score_random)\n",
    "\n",
    "tree_depth = DecisionTreeRegressor(criterion='absolute_error', splitter='best', min_samples_leaf=0.2, min_weight_fraction_leaf=0.1)\n",
    "tree_depth.fit(x_train_cali, y_train_cali)\n",
    "tree_depth.predict(x_train_cali)\n",
    "score_depth = tree_depth.score(x_test_cali, y_test_cali)\n",
    "print(\"max depth score:\", score_depth)\n",
    "\n",
    "tree_S_leaf = DecisionTreeRegressor(criterion='absolute_error', max_depth=25, splitter='best', min_weight_fraction_leaf=0.1)\n",
    "tree_S_leaf.fit(x_train_cali, y_train_cali)\n",
    "tree_S_leaf.predict(x_train_cali)\n",
    "score_S_leaf = tree_S_leaf.score(x_test_cali, y_test_cali)\n",
    "print(\"sample leaf score:\", score_S_leaf)\n",
    "\n",
    "tree_Wf_leaf = DecisionTreeRegressor(criterion='absolute_error', max_depth=25, splitter='best', min_samples_leaf=0.2)\n",
    "tree_Wf_leaf.fit(x_train_cali, y_train_cali)\n",
    "tree_Wf_leaf.predict(x_train_cali)\n",
    "score_wf_leaf = tree_Wf_leaf.score(x_test_cali, y_test_cali)\n",
    "print(\"weight fraction leaf score:\", score_wf_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "classifiers = ['tree_fried', 'tree_poss', 'tree_square', 'tree_random', 'tree_depth', 'tree_S_leaf', 'tree_Wf_leaf']\n",
    "time = [0.4355282403885572, 0.4210388274465735, 0.43552824038855686, 0.28965558197665864, 0.42182084923606145, 0.5211093178860082, 0.42182084923606145]\n",
    "\n",
    "x_pos = [i for i, _ in enumerate(classifiers)]\n",
    "colours = ['green', 'red', 'blue', 'black', 'orange', 'grey', 'yellow']\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.bar(x_pos, time, color=colours)\n",
    "plt.xlabel(\"different Trees\")\n",
    "plt.ylabel(\"accuracy score against testing data\")\n",
    "plt.title(\"All Tree Models Compared\")\n",
    "\n",
    "plt.xticks(x_pos, classifiers)\n",
    "\n",
    "plt.savefig('trees_compared.jpeg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every model, all training data sets were used to train the model and they were tested against the test target values. From these outputs and the barchart, The tuned parameters with default value for min_sample_leaf, gives the best accuracy. The worst performing tree model is the hyperparameter set to \"random\" as the splitter. Here the nodes are split at random and severely impact the accuracy of the model. Therefore, the hyperparameter with strongest effect is our splitting strategy, which here the \"best\" gives more consistant and better accuracy. Aside from these two hyperparameters, others did not noticeably impact the accuracy score.\n",
    "\n",
    "2.4.1.3: Timings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.2.1:\n",
    "\n",
    "Ensemble method is a technique used to combine different models produced on a dataset to furthure improve the results since they are more accurate than just a single model. Three are three main classes of ensemble learning methods: bagging, stacking, and boosting. Bagging utilises different samples of training set, to train a number of different models with replacement, then it combines predictions by taking the mean prediction of each model. \n",
    "\n",
    "(EDIT THIS)\n",
    "Boosting involves training base models in sequence, to ensure that each base model addresses the weaknesses of the ensemble. Instead of training a new base model on a random sample, we weight the data points in the training set according to the performance of previous base models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aab5caecb893b8334d552be4bd69e755905b7c3ba01386222cec6b7f9a909f6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
