{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import gzip\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib.colors import LogNorm\n",
    "from scipy.stats import multivariate_normal\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load minst copied from the fashion-mnist datasets's utils. This method is used to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    \n",
    "    ''' from https://github.com/zalandoresearch/fashion-mnist'''\n",
    "\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle components analysis or PCA, is a technique for reducing the dimensionality of datasets. The aim is to increase a databases interpretability while minimizing information loss. This is achieved by creating new uncorrelated variables that successively maximize variance. in the cell below, the dataset is loaded and then standardized by dividing them by 225 since they are all pixel values. Furthermore, their mean is calculated, and the data is normalized to make the PCA appliance easier. The first 25 images are shown with matching labels in the block below. Method 'conv_2d' splits the 1d input of 784 pixels into 2d with the shape (28,28) to make the plotting possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_X_train, train_y_labels = load_mnist('C:/Uni/Year3/MLCW//fashion-mnist/data/fashion', kind='train')\n",
    "fashion_X_test, test_y_labels = load_mnist('C:/Uni/Year3/MLCW/fashion-mnist/data/fashion', kind='t10k')\n",
    "\n",
    "class_names = ['t-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'bag', 'ankle boot']\n",
    "\n",
    "#standardization: devided by 255\n",
    "x_train_standard = fashion_X_train/255\n",
    "x_test_standard = fashion_X_test/255\n",
    "\n",
    "#normalizing by finding mean and calculating distance from mean\n",
    "train_mean = np.mean(x_train_standard)\n",
    "test_mean = np.mean(x_test_standard)\n",
    "\n",
    "train_normal = x_train_standard - train_mean\n",
    "test_normal = x_test_standard - test_mean\n",
    "\n",
    "\n",
    "def conv_2d(matrix):\n",
    "    for x in matrix:\n",
    "        matrix = np.reshape(matrix, (28,28))\n",
    "    return matrix\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    train_images = conv_2d(fashion_X_train[i])\n",
    "    plt.imshow(train_images, cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_y_labels[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block, the dimensionality of the data is reduced to only two components from 784. PCA is performed here to understand whether this reduction significantly impacts our information retention or not. a good reduction will aim to keep most of the information while discarding as many dimensions as possible. Here, we learn some quantities about these data namely different components and cumulative variance of the components. These represent the principal components of the data. in the code cell below evector of >=0.8 is taken from the cumulative variance as we want to retain 80+% of our datapoints as it is the ideal number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "fit = pca.fit_transform(train_normal)\n",
    "\n",
    "cumilative = np.cumsum(pca.explained_variance_)\n",
    "\n",
    "print(f\"first 5 PCs: {pca.explained_variance_[:10]}\")\n",
    "\n",
    "\n",
    "ex_var_ratio = pca.explained_variance_ratio_\n",
    "cum_var_ratio = np.cumsum(ex_var_ratio)\n",
    "first_evec_90 = np.where(cum_var_ratio >= 0.8)[0][0] #evector with cumulative variance ratio of >=0.8\n",
    "\n",
    "# Plot cumulative explained variance ratio\n",
    "principle_components = list(range(1, len(ex_var_ratio)+1))\n",
    "plt.plot(principle_components, np.cumsum(ex_var_ratio), color= 'black')\n",
    "plt.annotate(\"PC = \"+str(first_evec_90+1)+\", CVR = 80%\", (80, 0.9)).set_fontsize(15)\n",
    "plt.xlabel('Kth Principal Component')\n",
    "plt.ylabel('Cumulative Variance Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, the PCA was applied to all data and the pca object of the first 10 principles were returned. With this analysis we intend to keep higher variances. The output tells us that the cumalitive variance decreases rapidly. Furthermore, to understand how many dimension would be ideal to retain, Cumulative Variance Ratio per Principal Component is plotted. This better visualizes the pca of whole data. It can be observed that the first \"24\" principle components can explain 80% of the variance in this dataset. so reducing the datasets to 24 dimensions is ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the cell below we reduce the dimensionality of our data to 2 components. the data is then visualized as we plot the first component against the second component. Cumulative variance of the two components are also printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2 = PCA(n_components=2)\n",
    "fit_2d = pca_2.fit_transform(train_normal)\n",
    "cum_var = pca_2.explained_variance_#cumilitive variance\n",
    "cumalitive_var = np.cumsum(cum_var)\n",
    "print(cumalitive_var)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(fit_2d.T[0], fit_2d.T[1], c=train_y_labels)\n",
    "clb = plt.colorbar(scatter) #colour coding\n",
    "clb.ax.set_title('Class') #colour coding\n",
    "plt.xlabel(\"1st principle comp\", fontsize=12)\n",
    "plt.ylabel(\"2nd principle comp\", fontsize=12)\n",
    "plt.title('2D pca')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first principle component is \"19.80980567\" and the second principle component is \"31.92201614\" clearly showing the gap between the two components are high. This tells us that reducing to 2d from 780+ dimensions is not a good idea since all the classes keep clustering into each other and there is no clear separation between the classes and most of the datapoint are lost and we are underfitting our data. We can furthur prove that this reduction is not fitting with plotting our images with this reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the grid of images\n",
    "fig, axes = plt.subplots(2,2, figsize = (10,5), sharex=True, sharey = True, constrained_layout = True )\n",
    "\n",
    "# First 2 principal components\n",
    "f_2_PCS = [pca.components_[i].reshape(28,28) for i in range(0,2)]\n",
    "\n",
    "for pc in range(0,2):\n",
    "    axes[0,pc].imshow(f_2_PCS[0], aspect='auto', cmap = \"gray_r\")\n",
    "    axes[1,pc].imshow(f_2_PCS[pc], aspect='auto', cmap = \"gray_r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is demonstrated that the images come out extremly blurry and do not really represeant all of the shapes we will see within our dataset. Moreover, these images show that most variance in the data is found between the pixel values of a dark shirt and the pixel values of a white shoe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the code block below a simple K-means clustering is being employed. Each data is being assigned a cluster and cluster centres are calculated and marked inside each cluster. This is an unsupervised task with the task to separate points inside a dataset into different clusters such that elements within each cluster are similar. GMM is an example of soft clustering while K-mean is an example of hard clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10)\n",
    "label = kmeans.fit_predict(fit_2d)\n",
    "centroid = kmeans.cluster_centers_\n",
    "unique = np.unique(label)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in unique:\n",
    "    plt.scatter(fit_2d[label == i , 0] , fit_2d[label == i , 1] , label = i)\n",
    "    plt.scatter(centroid[:,0], centroid[:,1], s = 80, color = 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. The probability of where each datapoints belong to and plot them below, since GMM is a probabilistic model we use them to fit and find clusters for a number of our data points by using the fit_predict() method and scatter the datapoints with colours corresponding with their assigned clusters. Furthermore, the cluster centroids are represented by a grey circle within the scatters. z-score for this model is \"-5.041879030344901\" with some minor overlapping happening between the classes. Keeping in mind only 300 sample data were used for optimization. This score reveals that these datapoints are about 5 standard deviations away from the mean. Data's standard deviation is \"4.017053219615373\" while the mean is \"0.08295799598790543\". Since there is a large gap, it suggests that our data are scattered and not close to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours = ['red', 'green', 'purple', 'yellow', 'blue', 'cyan', 'black', 'grey', 'orange', 'brown']\n",
    "\n",
    "gmm = GaussianMixture(n_components=10, covariance_type='full', random_state=0)\n",
    "x = fit_2d[:300,:300]\n",
    "mea = gmm.fit(x).means_\n",
    "means = np.mean(mea)\n",
    "labels = gmm.fit_predict(x)\n",
    "score = gmm.score(x)\n",
    "std = np.std(x)\n",
    "print('std', std)\n",
    "print('mean', means)\n",
    "print('score', score)\n",
    "\n",
    "centers = np.empty(shape=(gmm.n_components, x.shape[1]))\n",
    "mean_centre = gmm.means_\n",
    "cov_centre = gmm.covariances_\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(gmm.n_components):\n",
    "    density = multivariate_normal(mean_centre[i], cov_centre[i]).logpdf(x)\n",
    "    centers[i, :] = x[np.argmax(density)]\n",
    "plt.scatter(centers[:, 0], centers[:, 1], s=500, c='grey', marker='X')\n",
    "pred = plt.scatter(x[:, 0], x[:, 1], c=labels, cmap='viridis', s=20) #predictions\n",
    "clb = plt.colorbar(pred) #colour coding\n",
    "clb.ax.set_title('Class labels') #colour coding\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code cell below plots and compares our actual datapoints against the prediction from the Gaussian mixture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aab5caecb893b8334d552be4bd69e755905b7c3ba01386222cec6b7f9a909f6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
